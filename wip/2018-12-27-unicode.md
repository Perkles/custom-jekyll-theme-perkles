---
date: 2018-12-20
title: O minimo que todo desenvolvedor deveria saber sobre unicode (Sem desculpas!)
layout: posts
author: Otávio Reis Perkles
comments: true
---

![Pilates](/assets/images/header-pilares.png){:class="title-image"}

___

https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/

Ja se perguntou sobre aquela tag misteriosa 'Content-Type'? Voce sabe, aquela que voce deveria por no HTML e nunca soube realmente o que era ?

Voce ja recebeu um email de um amigo da Bulgaria com o título “???? ?????? ??? ????”?

Fico desanimado em discobrir o quão pouco familiarizados os desenvolvedores  estão com o misterioso mundo dos _character sets_, _encodings_, _Unicode_, todas essas coisas. Alguns anos atraz, um beta teste da [FogBUGZ] andava se perguntando se poderia lidar com emails recebidos em japones. Japones ? Eu nao fazia ideia. Quando examinei mais de perto o controler ActiveX que estavamos usando para transformar MIME emails, descobrimos que ele estava fazendo errado o conjunto de caracteres, então nós teriamos de escrever um código heróico para desfazer a conversão errada e refaze-la corretamente. Quando olhei em outra bibliotec comercial, ela , também continha um código de implementação de characteres completamente quebrado. Entrei em contato com o desenvolvedor daquele módulo e ele me disse por alto '_não posso fazer nada por você_'. Como muitos programadores, nós apenas desejamos que tudo explodisse de alguma forma.

Mas nao explodiu. Quando eu descobri que a popular ferramenta de desenvolvimento web PHP ignorava quase que completamente problemas de codificação , brilhantemente usando 8 bits para os characteres, tornando quase impossível desenvolver aplicações web a nível internacional, eu pensei: _agora chega_

Então eu tinha um pronunciamento a fazer: Se você é um programador trabalhando em 2003 você não sabe o basico de _characteres_, _characteres set_, _encodings_, e _Unicode_ e eu vou te pegar, Eu fazer voce descascar cebolas em um submarino durante 6 meses como punição. Eu juro que vou.

E mais uma coisa:

*Não é tão difícil*

Nesse artigo eu vou lhe mostrar exatamente o que _todo programador deveria saber_. Todas as coisas sobre "plain text=ascii=characters são 8 bits" não é apenas errado, é irremediavelmente errado, e se você ainda programa dessa forma, voce nao é melhor do que um medico que nao acredita em germes. Por favor nao escreva outra linha de código antes de terminar de ler esse artigo.

Antes de começar, devo avisar que se você é uma daquelas raras pessoas que conhecem sobre internacionalização, voce vai achar toda essa discussão um pouco simples de mais. Estou apenas tentanto estabelecer o minimo para que vc entenda o que esta rolando e para que voce possa escrever código que funcione com linguagem de texto além do subgrupo do ingles, que não inclui palavras com acento. E eu devo te alertar que lidar com characteres é apenas uma pequena porção do que é requerido para criar programas que funcionam internacionalmente, mas posso escrever apenas uma coisa de cada vez, então hoje falaremos sobre conjunto de ccaracteres.

*Uma perspectiva hisótica*

A forma mais fácil de entender isso é seguindo cronologicamente.

Voce provavelmente acha que eu vou falar sobre conjuntos de caracteres antigos como EBCDIC. Bem , eu nao vou. EBCDIC nao é relevante ára sua vida. Nos não temos de voltar tanto no tempo.

De volta a nosso passado recente, quando UNIX estava sendo inventado e K&R era escrito na linguagem de programação C, tudo era muito simples. EBDCIC era a saida. Os unicos caracteres que importavam eram as velhas e boas desacentuadas letras americanas, e nos tinhamos codigo para elas chamado ASCII, no qual era possível representar qualquer caractere usando um numero entre 32 e 127. Espaço era 32, a letra 'a' era 65, etc. Isso, convenientemente, poderia ser guardado em 7 bits. A maioria dos computadores naquele tempo estavam usando bytes de 8-bits, então nao apenas era possível guardar todos os caracteres ASCII possíveis, mas voce tinha um bit inteiro sobrando, no qual, se você fosse fraco, voce poderia usar para suas próprias propostas diabólicas. Códigos abaixo de 32 eram chamados _unprintable_ e eram usados para maldições. Brincadeira. Eles eram usados como caracteres de  quye controle, como o 7 que fazia o computador apitar e o 12 que fazia com que a pagina autal da impressoa fosse ejetada e uma nova página tomasse seu lugar.

Tudo ia bem, assumindo que você era um falante de ingles.

Porque bytes contém espaço para 8 bits, muitas pessoas se pegavam pensando: "_Deus, nós podemos usar os códigos 128-255 para o que bem entendermos_". O prolema era, muitas pessoas tiveram essa mesma ideia ao mesmo tempo, e suas próprias concepções para o que deveria preencher o espaço entre 128 e 255. O **IBM-PC** tinha algo que ficou conhecido como _OEM character set_ que provinha alguns caracteres acentuados para lingua européia e um punhado de linhas ... barras horizontais, barras veritcais com penduricalhos no lado direito, etc... e você poderia usar essas linhas para desenenhar caixas simples. De fato assim que pessoas de fora da Ameria começaram a comprar computadores, todo tipo de OEM apareceu, nos quais todos usavam os numeros após 128 para seus próprios propósitos. Por exemplo em algum PC o código 130 pode exibir a letra 'é' mas em computadores vendidos em Israel contém a letra hebraica _Gimel_ (ג), então quando americanos mandavam seus _résumés_ para israel eles chegavam como rגsumגs. Em muitos casos, como o Russo, eles tinha muitas ideias diferentes do que fazer com os caracteres acima de 128, o que impossibilitava ate mesmo a troca de documentos dentro do país.

Eventuamlemte essa OEM 'para todos' foi certificada sobre o selo ANSI. Sobre o selo ANSI, todos concordavam com o que fazer com o exedente 128, o que era quase a mesma coisa do ASCII, mas haviam muitas formas diferentes de lidar com os caracteres 128 e acima, dependendo de onde voce vive.
Esses sistemas diferentes eram chamados de [_code pages_]. Então por exemplo em Israel DOS usava um _code page_ chamado 862, enquanto usuarios gregos usavam 737. Eles eram os mesmos abaixo de 128 mas diferentes acima de 128, onde todas as letras engraçadas residiam. A versão nacional do MS-DOS continham duzias dessas _code pages_, lidando com tudo do Ingles até o _Sorvetês_ e eles tinham ate meso algumas _code pages_ 'multilingues' que conseguiam fazer Esperanto e Galego no mesmo computador! WOW! Mas ter Hebraico e Grego no mesmo computador era impossível a nao ser que voce escrevesse seu proprio programa para mostrar tudo usando graficos bitmapeados, porque Hebraico e grego requerem tipos diferentes de code pages com tipos diferentes de interpretações para os numeros acima de 128.

Enquanto isso, na Asia, coisas ainda mais doidas estavam rolando uma vez que o alfabeto Aziatico contém centenas de letras que nunca iriam caber em 8-bits. Isso era geralmente resolvido pelo confuso systema chamado DBCS, o "_double byte character set_" nos quais _algumas_ letras eram gravadas em um bite e outras consumiam dois. Era mais facil seguir em uma string, mas era quase impossível voltar. Programadores eram encorajados e nao usar s++ e s- para se mover para traz e para frente, mas chamar funções como Windows’ AnsiNext and AnsiPrev ao invés para lidar com toda a bagunça.

Ainda assim, a maioria das pessoas apenas fingiam que um byte continha um caractere e que um caractere continha 8 bits e contando que voce nunca levasse aquela string de um computador para outro, ou falasse outra lingua, ia meio que sempre funcionar. Mas é claro, assim que a internet aconteceu, virou meio que senso comum mover string de um computador para outro e a coisa toda começou a desmoronar. Por sorte o unicode foi inventado.

**Unicode**

Unicode fazia um bravo esforço de criar um unico conjunto de caracteres para incluir qualquer sistema de escrita no planeta, ate mesmo os inventados como o Klingon. Algumas pessoas tem a microconcepção de que Unicode é simplesmente código de 16-bit onde cada caractere consome 16 bits logo temos 65,536 possíveis caracteres. **Não é bem assim.** Esse é um mito mais comum sobre Uicode, então se você pensou nisso, não se sinta mal.

Na verdade Unicode tem um jeito diferente de pensar sobre os caracteres, e voce tem de entender o jeito Unicode de ver as coisas ou nada fará sentido.

ATe agora, nos assumimos que uma letra é mapeada para alguns bits que podem ser guardados em algum disco na memória:

A -> 0100 0001

Em Unicode, uma letra é maeada para algo chamado de _code point_ que ainda assim é apenas um conceito teórico. Como esse _code point_ é representado na memoria ou em disco é uma outra história maluca.

Em Unicode, a letra A é um ideal platonico. É apenas uma núvem no céu.

**A**

Esse **A** platônico é diferente do **B**, e diferente do **a**, mas igual a **A** e _**A**_. A ideia que 'A' na fonte Times New Roman é o mesmo caractere que 'A' na fonte Helvetica, mas diferente de 'a' em minúsculo, não parece controverso, mas em algumas linguágens apenas descobrir o que a letra _é_ pode causar controversia. A letra ß em alemão é ume letra de verdade ou so um jeito bonitinho de escrever ss ? Se a forma da letra muda no final da palavra, é uma letra diferente ? Hebraico diz que sim, Árabe diz que não. De qualquer forma, as pessoas inteligentes por traz do Unicode vem pensando nisso pelomenos pela ultima década, acompanhados de um grande debate político no qual você nao precisa nem se preocupar. Eles ja descobriram.

Cada letra platonica em todo alfabeto é acompanhada de um numero magico como: **U+0639**. Esse numero magico é chamado de _code point_. O U+ significa "Unicode" e os numeros são hexadecimais. **U+0639** é a letra árabe Ain. A letra A em inglês é **U+0041**. Você pode achar todas visitando [o site do Unicode]().

Não existe limite real no numero de letras que o Unicode pode definir, na verdade deles foram além de 65,536 então nem toda letra unicode pode ser espremida em dois bytes, mas isso é um mito de qualquer forma.

Ok , vamos supor que temos uma string:

**Hello**

no qual em unicode corresponde a esses cinco _code poits_:

U+0048 U+0065 U+006C U+006C U+006F.

Apenas um punhado de code points. NUmeros, na verdade. Nós nem falamos nada sobre como salvar isso em memória ou representar isso em uma mensagem de email.

**Encodings**

É ai que o _encoding_ entra em ação.

A ideia primaria para a codificação Unicode, o que leva ao mito sobre dois bytes, era: vamos apenas armazenas esses numeros em dois bytes cada. Então Hello vira:

00 48 00 65 00 6C 00 6C 00 6F

Certo ? ainda não! Ainda pode ser:

48 00 65 00 6C 00 6C 00 6F 00 ?

Bem , tecnicamente sim ,eu acredito que poderia, de fato, as primeiras pessoas que começaram a implementar o Unicode queriam poder guardar seus _code points_ em modo [high-endian or low-endian](https://pt.wikipedia.org/wiki/Extremidade_(ordena%C3%A7%C3%A3o) em qualquer que fosse sua CPU, se fosse de manhã ou de tarde, teria de haver dois jeitos de salvar Unicode. Então as pessoas foram forçadas a inventar a bizarra convensão de colocar FF EE no inicio de toda String Unicode. Isso é chamado [Unicode Byte Order Mark]() e se você trocar entres seus hight-bytes e low-bytes vai obter algo parecido com FF FE e a pessoa lendo sua string que a troca foi executada.
Nem toda string unicode mundo a fora tem marcação da ordem de byte no inicio.

[image]

Por um tempo pareceu ser bom o suficiente, mas programadores começaram a reclamar. 'Da uma olhada nesses zeros!' eles falavam, como americanos eles estavam olhando para texto em ingles que raramente usavam pontos acima de U+00FF. Então a maioria das pessoas decidiram ignorar Unicode por varios anos e nesse meio tempo as coisas pioraram.

Então foi [inventado]() o conceito brilhante do [UTF-8](). UTF-8 foi outro sistema para armazenar aqueles numeros mágicos U+ das string Unicode em memória usando 8 bit bytes. Em UTF-8, todo _code point_ do 0-127 é guardado em um unico byte. Apenas _code points_ acima de 128 são armazenados usando 2,3, chegando ate 6 bytes.

[imagem]

Isso tinha o conveniente efeito colateral no qual textos em ingles permaneciam os mesmo em UFT-8 dos em ASCII, então americanod nem mesmo notariam algo diferente. Apenas o resto do mundo deveria fazer acrobacias. Especificamente, **Hello**, que era U+0048 U+0065 U+006C U+006C U+006F, seria guardado como 48 65 6C 6C 6F, YAY!! É o mesmo que seria armazenado em ASCII, ANSI, e qualquer outro conjunto de caractere OEM no planeta. Agora, se voce é ousado o suficiente para usar letras acentuadas, ou gregas ou em klingon, voce vai ter que usar varios bytes para guardar um unico _code point_, mas os Americanos nunca irão notar.

Ate agora eu lhe disse tres formas de codificar Unicode. O tradicional metodo guardar-em-dois-bytes é chamado UCS-2 (porque tem dois bytes) ou UTF-16 (porque tem 16 bytes), e voce ainda tem de adivinhar se é high-endian UCS-2 ou low-endian UCS-2. E tem o novo padrão UTF-8 que tem a pripriedade de trabalhar respeitavelmente se voce tem a feliz coicidencia de lidar com texto em ingles. Programas proprietários são completamente alheios ao fato de que existe qualquer coisa alem de ASCII.

Existem atualmente um punhado de outras formas de codificar Unicode. Tem o chamado UTF-7, que é bem parecido com UTF-8 mas garante que os 'hight_bits' sempre sejam zero, então se voce tem que passar Unicode por algum tipo de sistema de email draconiano que acha que 7 bits é o suficiente voce acaba saindo ilezo por pouco.

Agora que voce esta pensando dentro dos termos das letras platonicas ideais que são representadas por code points, esse unicode code point também pode ser codificado usando qualquer codificação _old-school_. Por exemplo, voce pode codificar a string Hello em unicode (U+0048 U+0065 U+006C U+006C U+006F) em ASCII, ou no antigo OEM Greek, ou Hebraico ANSI, ou qualquer um dois muitos outros sistemas de codificação que foram inventados ate agora com uma coisa em mente: Algumas das letras podem nao aparecer! Se nao existtir um code point equivalente a codificação que voce esta tentando representar, geralmente você obtém um ponto de interrogação: ?, ou se você for muito bom uma caixa. Qual você tirou ?  -> �

Existem centenas de codificações tradicionais que podem apenas guardar alguns code points de forma correta e mudar todos os outros code points para pontos de interrogação. Algumas das codificações popular do Ingles são Windows-1252 (O Windows 9x padrão para linguas ocidentais européias) e ISO-8859-1, também conhecido como Latin-1 (muito útil também para quelquer lingua ocidental europeia). Mas tente guardar letras em  Russo ou Hebraico com essas codificações e voce tera um monte de pontos de interrogação. UTF-7,8,16 E 32 todos tem a ótima propriedade de serem capazes de guardar _qualquer_ codepoint de forma correta.

**O unico fato importante sobre Encodings**

Se voce esqueceu completamente o que eu acabei de explica, por favor lembre um fato extreemamente importante. **Não faz sentido ter uma string sem saber para que encoding é usado**. Voce nao pode mais enfiar sua cabeça naareia e fingir que texto "liso" é ascii

**Não existe isso de texto liso**

Se você tem uma string na memoria, em um arquivo , ou em uma mensagem de meial, voce tem que sabe que codificação esta contida ali ou voce nao poderá interpleta-la e exibi-la para o usuario corretamente.

Quase toda frase estupida como "Meu site parece um rascunho" ou "Ela nao consegue ler meus emails quando eu coloco acento" problemas vem de um programador timido que nao entendeu o simples fato que
